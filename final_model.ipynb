{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f20e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import gc # Garbage Collection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca17c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"D:/PythonDataSci/cdc project/data/processed/X_train.npy\")\n",
    "y = np.load(\"D:/PythonDataSci/cdc project/data/processed/y_train.npy\")\n",
    "\n",
    "X_test  = np.load(\"D:/PythonDataSci/cdc project/data/processed/X_test.npy\")\n",
    "test_ids = np.load(\"D:/PythonDataSci/cdc project/data/processed/test_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbaf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ac041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3654b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\awate\\.conda\\envs\\ml_env\\lib\\site-packages\\xgboost\\core.py:729: UserWarning: [18:45:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.02, max_depth=5, device='cuda',n_jobs=-1)\n",
    "xgb.fit(X_train, y_train_log)\n",
    "y_prd_log = xgb.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98317820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 16:49:40,764] A new study created in memory with name: no-name-b9859f81-bad0-4a9e-89bb-45be942e8353\n",
      "[I 2025-12-30 16:50:08,298] Trial 0 finished with value: 117098.85710800084 and parameters: {'learning_rate': 0.02335863963375386, 'max_depth': 5, 'colsample_bytree': 0.6935916160955015, 'subsample': 0.8613529447924582, 'reg_alpha': 1.1012454434131733, 'reg_lambda': 0.10474540040495997}. Best is trial 0 with value: 117098.85710800084.\n",
      "[I 2025-12-30 16:51:06,869] Trial 1 finished with value: 115783.07551624287 and parameters: {'learning_rate': 0.0060297511801735825, 'max_depth': 7, 'colsample_bytree': 0.5864705011664011, 'subsample': 0.6295304002577121, 'reg_alpha': 1.916964949702383e-07, 'reg_lambda': 0.47407472304305526}. Best is trial 1 with value: 115783.07551624287.\n",
      "[I 2025-12-30 16:51:50,897] Trial 2 finished with value: 115058.13487102944 and parameters: {'learning_rate': 0.005126653039481932, 'max_depth': 6, 'colsample_bytree': 0.7196040589241918, 'subsample': 0.6165747965239472, 'reg_alpha': 0.002105792041297527, 'reg_lambda': 5.07837675545107e-05}. Best is trial 2 with value: 115058.13487102944.\n",
      "[I 2025-12-30 16:52:03,530] Trial 3 finished with value: 125459.99432488429 and parameters: {'learning_rate': 0.0056272304728112125, 'max_depth': 3, 'colsample_bytree': 0.771385549962224, 'subsample': 0.7060636178302492, 'reg_alpha': 1.7841486583779245, 'reg_lambda': 1.7255871218684438e-05}. Best is trial 2 with value: 115058.13487102944.\n",
      "[I 2025-12-30 16:52:52,831] Trial 4 finished with value: 116049.06217630541 and parameters: {'learning_rate': 0.01631429228063415, 'max_depth': 6, 'colsample_bytree': 0.8649761369045099, 'subsample': 0.8861071946960051, 'reg_alpha': 3.0065900715629014e-08, 'reg_lambda': 0.16197896677380744}. Best is trial 2 with value: 115058.13487102944.\n",
      "[I 2025-12-30 16:53:48,593] Trial 5 finished with value: 114889.5193827531 and parameters: {'learning_rate': 0.023359242933840348, 'max_depth': 6, 'colsample_bytree': 0.7960088237241815, 'subsample': 0.6472374012867489, 'reg_alpha': 1.6849316864657289e-07, 'reg_lambda': 0.0535196478987071}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:55:18,833] Trial 6 finished with value: 117207.94075488231 and parameters: {'learning_rate': 0.024423622796200296, 'max_depth': 7, 'colsample_bytree': 0.6957300416352843, 'subsample': 0.728127706048205, 'reg_alpha': 4.091364698639411e-06, 'reg_lambda': 0.000321860610403713}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:56:26,875] Trial 7 finished with value: 118198.86050212159 and parameters: {'learning_rate': 0.025350699121648522, 'max_depth': 7, 'colsample_bytree': 0.46156444135422053, 'subsample': 0.7700080565851912, 'reg_alpha': 0.00039641140484148043, 'reg_lambda': 3.082351667006255e-08}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:56:45,435] Trial 8 finished with value: 118971.54160554531 and parameters: {'learning_rate': 0.01671886005939993, 'max_depth': 3, 'colsample_bytree': 0.4155071231651808, 'subsample': 0.7306459493982196, 'reg_alpha': 0.007312189628902033, 'reg_lambda': 2.5213212218749046}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:57:05,415] Trial 9 finished with value: 117942.72345507373 and parameters: {'learning_rate': 0.027204776300025657, 'max_depth': 3, 'colsample_bytree': 0.8272759335839746, 'subsample': 0.7138093061167872, 'reg_alpha': 3.5499472271822802e-06, 'reg_lambda': 0.0014917909480184784}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:57:38,540] Trial 10 finished with value: 123288.04749853085 and parameters: {'learning_rate': 0.09361024597215113, 'max_depth': 5, 'colsample_bytree': 0.579588091542946, 'subsample': 0.6565054604609458, 'reg_alpha': 5.4271318723963205e-06, 'reg_lambda': 0.0064146902417460145}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:58:15,349] Trial 11 finished with value: 119364.16936417729 and parameters: {'learning_rate': 0.048367363966199034, 'max_depth': 5, 'colsample_bytree': 0.7750487433041069, 'subsample': 0.603371905731905, 'reg_alpha': 0.014682513616665427, 'reg_lambda': 6.028811212993874e-06}. Best is trial 5 with value: 114889.5193827531.\n",
      "[I 2025-12-30 16:59:18,787] Trial 12 finished with value: 113883.89989809798 and parameters: {'learning_rate': 0.010305153637326075, 'max_depth': 6, 'colsample_bytree': 0.7578755949153316, 'subsample': 0.6611922760426692, 'reg_alpha': 0.0003494030671459932, 'reg_lambda': 7.55036189272943e-06}. Best is trial 12 with value: 113883.89989809798.\n",
      "[I 2025-12-30 17:00:23,146] Trial 13 finished with value: 114568.59409105097 and parameters: {'learning_rate': 0.009145023358600003, 'max_depth': 6, 'colsample_bytree': 0.8945823959360192, 'subsample': 0.6729802319144268, 'reg_alpha': 4.26972888547694e-05, 'reg_lambda': 2.417991837802597e-07}. Best is trial 12 with value: 113883.89989809798.\n",
      "[I 2025-12-30 17:00:49,963] Trial 14 finished with value: 117378.52123791643 and parameters: {'learning_rate': 0.008843888199319298, 'max_depth': 4, 'colsample_bytree': 0.893798299642281, 'subsample': 0.7956246615514381, 'reg_alpha': 8.162322299448849e-05, 'reg_lambda': 6.590981931437503e-08}. Best is trial 12 with value: 113883.89989809798.\n",
      "[I 2025-12-30 17:01:45,357] Trial 15 finished with value: 113316.92053704955 and parameters: {'learning_rate': 0.009904198901746396, 'max_depth': 6, 'colsample_bytree': 0.623938219025191, 'subsample': 0.674478606510761, 'reg_alpha': 0.07170126100189006, 'reg_lambda': 7.771416220844057e-07}. Best is trial 15 with value: 113316.92053704955.\n",
      "[I 2025-12-30 17:02:20,131] Trial 16 finished with value: 112277.49395137033 and parameters: {'learning_rate': 0.011178982956450563, 'max_depth': 5, 'colsample_bytree': 0.6034854651325706, 'subsample': 0.6854569745901181, 'reg_alpha': 0.09426738682137052, 'reg_lambda': 6.156082790201138e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:02:44,585] Trial 17 finished with value: 115809.89221996539 and parameters: {'learning_rate': 0.013782032359564348, 'max_depth': 4, 'colsample_bytree': 0.6028623458409188, 'subsample': 0.6902627103826084, 'reg_alpha': 0.13668327603929423, 'reg_lambda': 6.472378643009149e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:03:09,456] Trial 18 finished with value: 114556.01333845378 and parameters: {'learning_rate': 0.012136163402192443, 'max_depth': 4, 'colsample_bytree': 0.5152807295905596, 'subsample': 0.8084250168774347, 'reg_alpha': 0.09552955468079073, 'reg_lambda': 1.0079574056799829e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:03:44,470] Trial 19 finished with value: 113847.217199192 and parameters: {'learning_rate': 0.0075852847419120625, 'max_depth': 5, 'colsample_bytree': 0.6379112735762704, 'subsample': 0.7617735268822281, 'reg_alpha': 0.21154142686367908, 'reg_lambda': 2.3397181544106486e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:04:18,408] Trial 20 finished with value: 117103.39117207495 and parameters: {'learning_rate': 0.04818285671866981, 'max_depth': 5, 'colsample_bytree': 0.5310675857191551, 'subsample': 0.6903660891367084, 'reg_alpha': 6.117376013715607, 'reg_lambda': 3.579212203273578e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:04:52,958] Trial 21 finished with value: 114291.47971743126 and parameters: {'learning_rate': 0.00726448625993493, 'max_depth': 5, 'colsample_bytree': 0.6417883581092331, 'subsample': 0.7644553859155521, 'reg_alpha': 0.08273777872596408, 'reg_lambda': 1.7967239313289643e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:05:18,112] Trial 22 finished with value: 118086.36947590522 and parameters: {'learning_rate': 0.0074948346182252545, 'max_depth': 4, 'colsample_bytree': 0.645488055062026, 'subsample': 0.8179020624338356, 'reg_alpha': 0.8421591149455185, 'reg_lambda': 9.638469676595096e-05}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:05:52,516] Trial 23 finished with value: 113293.72572212461 and parameters: {'learning_rate': 0.012318280741975255, 'max_depth': 5, 'colsample_bytree': 0.5342634238806422, 'subsample': 0.7509152672868957, 'reg_alpha': 0.027223040194696516, 'reg_lambda': 1.796683928841916e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:06:42,186] Trial 24 finished with value: 113355.03051916134 and parameters: {'learning_rate': 0.01226502491831954, 'max_depth': 6, 'colsample_bytree': 0.5292199716655748, 'subsample': 0.7347869117981823, 'reg_alpha': 0.017244007345239093, 'reg_lambda': 1.1173073784798967e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:07:15,514] Trial 25 finished with value: 113712.5887138271 and parameters: {'learning_rate': 0.01781689823883323, 'max_depth': 5, 'colsample_bytree': 0.487175984022761, 'subsample': 0.6857964494741179, 'reg_alpha': 0.001625525790190222, 'reg_lambda': 5.596553689133039e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:07:40,980] Trial 26 finished with value: 115596.26512997727 and parameters: {'learning_rate': 0.010764825929896289, 'max_depth': 4, 'colsample_bytree': 0.549005841575029, 'subsample': 0.6452794966602986, 'reg_alpha': 0.01995329873371236, 'reg_lambda': 3.2425649063932174e-05}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:08:32,972] Trial 27 finished with value: 114520.77987858797 and parameters: {'learning_rate': 0.0343764590761354, 'max_depth': 6, 'colsample_bytree': 0.6020084896798261, 'subsample': 0.7923381227535876, 'reg_alpha': 8.614425715907482, 'reg_lambda': 0.0004510807061653023}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:09:47,414] Trial 28 finished with value: 117799.64115395259 and parameters: {'learning_rate': 0.014515228111496532, 'max_depth': 7, 'colsample_bytree': 0.4499960329130528, 'subsample': 0.8378809343969302, 'reg_alpha': 0.0026309144040968743, 'reg_lambda': 2.8960274114877177e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:10:23,883] Trial 29 finished with value: 113368.73812475818 and parameters: {'learning_rate': 0.019058473703709708, 'max_depth': 5, 'colsample_bytree': 0.6949359350911454, 'subsample': 0.7062871634514505, 'reg_alpha': 0.7493112981673197, 'reg_lambda': 1.622427514080572e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:11:03,115] Trial 30 finished with value: 113396.49603052116 and parameters: {'learning_rate': 0.009433586959045769, 'max_depth': 6, 'colsample_bytree': 0.5630116273964989, 'subsample': 0.740586992542642, 'reg_alpha': 0.3507001887920467, 'reg_lambda': 1.0499421347883425e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:11:47,032] Trial 31 finished with value: 114563.3161531212 and parameters: {'learning_rate': 0.01411501594150804, 'max_depth': 6, 'colsample_bytree': 0.4916014632754209, 'subsample': 0.7191780632171575, 'reg_alpha': 0.06037760731256202, 'reg_lambda': 1.346124985102652e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:12:21,519] Trial 32 finished with value: 113542.13737639433 and parameters: {'learning_rate': 0.012135796328491773, 'max_depth': 5, 'colsample_bytree': 0.6089333800258991, 'subsample': 0.752112692263814, 'reg_alpha': 0.030497671390498044, 'reg_lambda': 1.0227104645580592e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:13:38,407] Trial 33 finished with value: 115497.38504399137 and parameters: {'learning_rate': 0.006603909887940414, 'max_depth': 7, 'colsample_bytree': 0.5284360956704283, 'subsample': 0.6308945589211734, 'reg_alpha': 0.005593746332138467, 'reg_lambda': 4.798093938016563e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:14:34,318] Trial 34 finished with value: 114315.00506932587 and parameters: {'learning_rate': 0.011806674880449276, 'max_depth': 6, 'colsample_bytree': 0.6640810930209307, 'subsample': 0.7810362887399279, 'reg_alpha': 1.6739576963951666, 'reg_lambda': 9.355278679419293e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:15:07,845] Trial 35 finished with value: 114613.25242745709 and parameters: {'learning_rate': 0.005414748097358861, 'max_depth': 5, 'colsample_bytree': 0.5606160321398529, 'subsample': 0.6722950325882823, 'reg_alpha': 0.0006874515045220354, 'reg_lambda': 6.683324341211265e-05}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:15:56,101] Trial 36 finished with value: 113872.3271036471 and parameters: {'learning_rate': 0.008535456947507705, 'max_depth': 6, 'colsample_bytree': 0.49540631445332345, 'subsample': 0.7496093751476225, 'reg_alpha': 0.031097335283753507, 'reg_lambda': 1.886258155790803e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:16:51,459] Trial 37 finished with value: 114404.63207405547 and parameters: {'learning_rate': 0.020515122551942915, 'max_depth': 6, 'colsample_bytree': 0.6711569754035389, 'subsample': 0.7042465501471755, 'reg_alpha': 0.30850901865351626, 'reg_lambda': 1.828751069235275e-05}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:18:22,251] Trial 38 finished with value: 114416.82673453237 and parameters: {'learning_rate': 0.006125231478906374, 'max_depth': 7, 'colsample_bytree': 0.7366410831654697, 'subsample': 0.7336974036392901, 'reg_alpha': 0.007409695975247257, 'reg_lambda': 3.075030151893045e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:19:03,523] Trial 39 finished with value: 114534.88886797769 and parameters: {'learning_rate': 0.014934764519137013, 'max_depth': 6, 'colsample_bytree': 0.6173032988260242, 'subsample': 0.8732724808774328, 'reg_alpha': 3.5268350754035915, 'reg_lambda': 1.0387867352258761e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:19:17,752] Trial 40 finished with value: 115599.3478182295 and parameters: {'learning_rate': 0.010712636218553789, 'max_depth': 4, 'colsample_bytree': 0.42515357186552394, 'subsample': 0.6774999945981043, 'reg_alpha': 0.00010544960211870799, 'reg_lambda': 4.590610946583576e-06}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:19:44,741] Trial 41 finished with value: 114466.96110231982 and parameters: {'learning_rate': 0.01935555511972492, 'max_depth': 5, 'colsample_bytree': 0.7105834484012641, 'subsample': 0.7025231255248884, 'reg_alpha': 0.7323979700986897, 'reg_lambda': 1.889910769999306e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:20:11,078] Trial 42 finished with value: 115127.26592775492 and parameters: {'learning_rate': 0.01684865463394621, 'max_depth': 5, 'colsample_bytree': 0.6927256797872767, 'subsample': 0.7210928645391974, 'reg_alpha': 0.48026270171773633, 'reg_lambda': 6.565216234558876e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:20:35,500] Trial 43 finished with value: 115495.52316864926 and parameters: {'learning_rate': 0.028348234440977328, 'max_depth': 5, 'colsample_bytree': 0.5716631300888784, 'subsample': 0.7025756983941039, 'reg_alpha': 1.4504958849745653, 'reg_lambda': 1.4415606205065388e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:21:01,032] Trial 44 finished with value: 114412.52183218408 and parameters: {'learning_rate': 0.02169281367162839, 'max_depth': 5, 'colsample_bytree': 0.6824662876740747, 'subsample': 0.744119974061542, 'reg_alpha': 0.05056089232827192, 'reg_lambda': 2.3165665113373313e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:21:35,941] Trial 45 finished with value: 113523.02518872549 and parameters: {'learning_rate': 0.012374808450181657, 'max_depth': 6, 'colsample_bytree': 0.6174942363369743, 'subsample': 0.6421050699164665, 'reg_alpha': 0.011530626467587457, 'reg_lambda': 8.196707935419939}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:22:00,520] Trial 46 finished with value: 114398.03075228175 and parameters: {'learning_rate': 0.015932955923392254, 'max_depth': 5, 'colsample_bytree': 0.58684018185878, 'subsample': 0.6624358778307948, 'reg_alpha': 0.0028973392781219704, 'reg_lambda': 0.01692252826058497}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:22:43,589] Trial 47 finished with value: 116068.64065715596 and parameters: {'learning_rate': 0.030403861759949496, 'max_depth': 6, 'colsample_bytree': 0.7314360946866664, 'subsample': 0.712871809904246, 'reg_alpha': 0.19918908955844453, 'reg_lambda': 4.126020392266627e-07}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:22:58,962] Trial 48 finished with value: 116031.98676227173 and parameters: {'learning_rate': 0.008310075398830497, 'max_depth': 4, 'colsample_bytree': 0.5457826650595788, 'subsample': 0.7757602715661331, 'reg_alpha': 0.0008654062164229127, 'reg_lambda': 8.946914802965716e-08}. Best is trial 16 with value: 112277.49395137033.\n",
      "[I 2025-12-30 17:23:21,357] Trial 49 finished with value: 114482.48997990915 and parameters: {'learning_rate': 0.010179365886679665, 'max_depth': 5, 'colsample_bytree': 0.46682090798370107, 'subsample': 0.6151548951060224, 'reg_alpha': 3.883919075448302, 'reg_lambda': 3.099810015104007e-08}. Best is trial 16 with value: 112277.49395137033.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    def clear_gpu():\n",
    "        torch.cuda.empty_cache()\n",
    "except ImportError:\n",
    "    def clear_gpu():\n",
    "        pass\n",
    "\n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    clear_gpu()\n",
    "    \n",
    "    param = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7), \n",
    "        'max_bin': 128, \n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),  \n",
    "        'device': 'cuda', \n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = xgb.XGBRegressor(**param)\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_valid)\n",
    "        rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        clear_gpu()\n",
    "        \n",
    "        return rmse\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with error: {e}\")\n",
    "        return float('inf') \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b29f6f",
   "metadata": {},
   "source": [
    "[I 2025-12-30 17:02:20,131] Trial 16 finished with value: 112277.49395137033 and parameters: {'learning_rate': 0.011178982956450563, 'max_depth': 5, 'colsample_bytree': 0.6034854651325706, 'subsample': 0.6854569745901181, 'reg_alpha': 0.09426738682137052, 'reg_lambda': 6.156082790201138e-07}. Best is trial 16 with value: 112277.49395137033.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8ee200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 1280 to 100\n"
     ]
    }
   ],
   "source": [
    "X_tabular = X_train[:, :22]\n",
    "X_embed   = X_train[:, 22:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_embed_scaled = scaler.fit_transform(X_embed)\n",
    "\n",
    "pca = PCA(n_components=100) \n",
    "X_embed_pca = pca.fit_transform(X_embed_scaled)\n",
    "\n",
    "print(f\"Reduced dimensions from {X_embed.shape[1]} to {X_embed_pca.shape[1]}\")\n",
    "\n",
    "X_train_final = np.hstack([X_tabular, X_embed_pca])\n",
    "\n",
    "X_valid_tabular = X_valid[:, :22]\n",
    "X_valid_embed   = X_valid[:, 22:]\n",
    "\n",
    "X_valid_embed_scaled = scaler.transform(X_valid_embed) \n",
    "X_valid_embed_pca    = pca.transform(X_valid_embed_scaled)\n",
    "\n",
    "X_valid_final = np.hstack([X_valid_tabular, X_valid_embed_pca])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2de70c",
   "metadata": {},
   "source": [
    "Using log preds with low dim pca embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf96b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-01 20:32:12,397] A new study created in memory with name: no-name-e824f5b6-d027-47b5-9b81-b025eedef0ee\n",
      "[I 2026-01-01 20:32:19,595] Trial 0 finished with value: 116552.13151204056 and parameters: {'learning_rate': 0.05918354950458916, 'max_depth': 5, 'colsample_bytree': 0.8722430587827523, 'subsample': 0.6540008601981402, 'reg_alpha': 7.77403451533842, 'reg_lambda': 2.1880436000663848e-06}. Best is trial 0 with value: 116552.13151204056.\n",
      "[I 2026-01-01 20:32:23,620] Trial 1 finished with value: 109098.43775233447 and parameters: {'learning_rate': 0.07900024150662363, 'max_depth': 3, 'colsample_bytree': 0.7831561338906451, 'subsample': 0.8373152944220122, 'reg_alpha': 0.003135473024115725, 'reg_lambda': 1.2477268748837542}. Best is trial 1 with value: 109098.43775233447.\n",
      "[I 2026-01-01 20:32:33,483] Trial 2 finished with value: 113221.69346905212 and parameters: {'learning_rate': 0.042056582591707865, 'max_depth': 6, 'colsample_bytree': 0.8250959362916875, 'subsample': 0.639247713795815, 'reg_alpha': 1.2594443652683716, 'reg_lambda': 4.984836478039343e-05}. Best is trial 1 with value: 109098.43775233447.\n",
      "[I 2026-01-01 20:32:39,543] Trial 3 finished with value: 107359.92846495382 and parameters: {'learning_rate': 0.017303550743086156, 'max_depth': 5, 'colsample_bytree': 0.5284497374225918, 'subsample': 0.8017560169898288, 'reg_alpha': 2.1770116583231706e-07, 'reg_lambda': 0.04016474474829754}. Best is trial 3 with value: 107359.92846495382.\n",
      "[I 2026-01-01 20:32:52,283] Trial 4 finished with value: 111663.45321545452 and parameters: {'learning_rate': 0.019522273686220313, 'max_depth': 7, 'colsample_bytree': 0.6303029513949001, 'subsample': 0.6213157155889996, 'reg_alpha': 4.182875266681497e-06, 'reg_lambda': 0.0024396552072454233}. Best is trial 3 with value: 107359.92846495382.\n",
      "[I 2026-01-01 20:33:06,921] Trial 5 finished with value: 113637.39328231706 and parameters: {'learning_rate': 0.03965830850190378, 'max_depth': 7, 'colsample_bytree': 0.8845795927954335, 'subsample': 0.7640415375070156, 'reg_alpha': 0.0017833150081436727, 'reg_lambda': 1.1103446527792984e-06}. Best is trial 3 with value: 107359.92846495382.\n",
      "[I 2026-01-01 20:33:12,951] Trial 6 finished with value: 105848.68656719364 and parameters: {'learning_rate': 0.025991488555589123, 'max_depth': 5, 'colsample_bytree': 0.4692948367223816, 'subsample': 0.8158348131145998, 'reg_alpha': 2.1487130607490056e-05, 'reg_lambda': 3.482964583992857e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:26,173] Trial 7 finished with value: 110914.61277938088 and parameters: {'learning_rate': 0.011938044927398521, 'max_depth': 7, 'colsample_bytree': 0.5930419721576083, 'subsample': 0.8950096487414847, 'reg_alpha': 0.5857410590560591, 'reg_lambda': 0.0633557148823915}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:35,080] Trial 8 finished with value: 109962.16018249186 and parameters: {'learning_rate': 0.006454892413837577, 'max_depth': 6, 'colsample_bytree': 0.6976064457627886, 'subsample': 0.6946501880278071, 'reg_alpha': 0.0415484009624851, 'reg_lambda': 4.832250351087402e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:40,414] Trial 9 finished with value: 106178.51714918607 and parameters: {'learning_rate': 0.07389554723965903, 'max_depth': 4, 'colsample_bytree': 0.8422730318118543, 'subsample': 0.7637461700897509, 'reg_alpha': 0.11029334696550352, 'reg_lambda': 1.2832554389915418e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:44,233] Trial 10 finished with value: 123944.01833085774 and parameters: {'learning_rate': 0.007608812163138234, 'max_depth': 3, 'colsample_bytree': 0.415778922355864, 'subsample': 0.8933608773989007, 'reg_alpha': 1.4478386558289809e-05, 'reg_lambda': 4.527843647683273e-05}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:49,121] Trial 11 finished with value: 106548.63122537051 and parameters: {'learning_rate': 0.034060177609452215, 'max_depth': 4, 'colsample_bytree': 0.44815795027511685, 'subsample': 0.728306508851192, 'reg_alpha': 7.650454888381e-05, 'reg_lambda': 2.8556281066655462e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:54,400] Trial 12 finished with value: 109574.68671184964 and parameters: {'learning_rate': 0.09397191740427592, 'max_depth': 4, 'colsample_bytree': 0.728252930144962, 'subsample': 0.8019838348578218, 'reg_alpha': 2.5384745190463334e-07, 'reg_lambda': 9.754606791890019e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:33:59,281] Trial 13 finished with value: 106658.97625610327 and parameters: {'learning_rate': 0.028026387725695896, 'max_depth': 4, 'colsample_bytree': 0.531977238849716, 'subsample': 0.8373012536451904, 'reg_alpha': 1.0393961902391725e-08, 'reg_lambda': 1.3416565278165242e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:34:10,652] Trial 14 finished with value: 106363.72554588335 and parameters: {'learning_rate': 0.05678541758203616, 'max_depth': 5, 'colsample_bytree': 0.5041815587870534, 'subsample': 0.7546931808865487, 'reg_alpha': 0.03697654352127757, 'reg_lambda': 6.802767386077437e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:34:23,802] Trial 15 finished with value: 108332.24627967428 and parameters: {'learning_rate': 0.01427607421722636, 'max_depth': 4, 'colsample_bytree': 0.7202073789822478, 'subsample': 0.7097957686325512, 'reg_alpha': 0.0004621028544921391, 'reg_lambda': 1.8927695699957597e-05}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:34:41,360] Trial 16 finished with value: 109086.28218066652 and parameters: {'learning_rate': 0.010509647297865181, 'max_depth': 6, 'colsample_bytree': 0.5929483299972131, 'subsample': 0.7907654345690208, 'reg_alpha': 0.03524183852171396, 'reg_lambda': 3.1337763224693255e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:34:58,079] Trial 17 finished with value: 108082.83638025049 and parameters: {'learning_rate': 0.024649849414857587, 'max_depth': 5, 'colsample_bytree': 0.7875324777681098, 'subsample': 0.8470278490320159, 'reg_alpha': 5.7359171324777245e-05, 'reg_lambda': 0.000296355011442518}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:35:11,213] Trial 18 finished with value: 109736.31464560854 and parameters: {'learning_rate': 0.057679697127903, 'max_depth': 3, 'colsample_bytree': 0.6740771233637136, 'subsample': 0.6745031275328665, 'reg_alpha': 1.3178880766880534e-06, 'reg_lambda': 8.7988794576025e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:35:25,712] Trial 19 finished with value: 112403.22842338649 and parameters: {'learning_rate': 0.09980112761523625, 'max_depth': 4, 'colsample_bytree': 0.45839434176953786, 'subsample': 0.7785946619083888, 'reg_alpha': 0.004253678892912153, 'reg_lambda': 1.1576927047830934e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:35:43,482] Trial 20 finished with value: 111895.1234683621 and parameters: {'learning_rate': 0.06911630380238543, 'max_depth': 6, 'colsample_bytree': 0.60163647470919, 'subsample': 0.8555836395134448, 'reg_alpha': 0.5950360107260401, 'reg_lambda': 0.0007537523568884773}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:35:58,569] Trial 21 finished with value: 106222.97212938452 and parameters: {'learning_rate': 0.04866636767929992, 'max_depth': 5, 'colsample_bytree': 0.5161910262785112, 'subsample': 0.7402991316122607, 'reg_alpha': 0.03908545332185849, 'reg_lambda': 1.9171634567764928e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:36:14,150] Trial 22 finished with value: 109532.32129376242 and parameters: {'learning_rate': 0.04655132409129308, 'max_depth': 5, 'colsample_bytree': 0.488863605035809, 'subsample': 0.7329568137214665, 'reg_alpha': 0.06312509155776545, 'reg_lambda': 6.460729569920949e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:36:30,630] Trial 23 finished with value: 105946.28314386494 and parameters: {'learning_rate': 0.029996727843866842, 'max_depth': 5, 'colsample_bytree': 0.4033032154272073, 'subsample': 0.821140147619962, 'reg_alpha': 0.0008457641241448405, 'reg_lambda': 1.0625472151313068e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:36:44,724] Trial 24 finished with value: 107625.40071934692 and parameters: {'learning_rate': 0.03144959569615524, 'max_depth': 4, 'colsample_bytree': 0.4000182288791787, 'subsample': 0.8174035135450527, 'reg_alpha': 0.00018399521486542052, 'reg_lambda': 1.6143046588001662e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:36:59,484] Trial 25 finished with value: 106134.08214141204 and parameters: {'learning_rate': 0.02193266862370329, 'max_depth': 5, 'colsample_bytree': 0.4500937547735985, 'subsample': 0.8184289012278199, 'reg_alpha': 0.0005793491877026303, 'reg_lambda': 4.43465743162345e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:37:16,286] Trial 26 finished with value: 110626.2817236483 and parameters: {'learning_rate': 0.0217867621426084, 'max_depth': 6, 'colsample_bytree': 0.4509862643596255, 'subsample': 0.8686253520925142, 'reg_alpha': 0.0005648528720812778, 'reg_lambda': 5.791385610672039e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:37:30,751] Trial 27 finished with value: 108285.94797110104 and parameters: {'learning_rate': 0.016245416371465805, 'max_depth': 5, 'colsample_bytree': 0.5631160322380271, 'subsample': 0.8213004196205914, 'reg_alpha': 2.4121998459989162e-05, 'reg_lambda': 0.00012799493864328238}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:37:44,423] Trial 28 finished with value: 106619.5677350082 and parameters: {'learning_rate': 0.02452413259371128, 'max_depth': 5, 'colsample_bytree': 0.4295981813879402, 'subsample': 0.8724938737523418, 'reg_alpha': 4.924394300905492e-06, 'reg_lambda': 3.8079898173463116e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:37:59,677] Trial 29 finished with value: 109311.00702125106 and parameters: {'learning_rate': 0.009647414630606409, 'max_depth': 5, 'colsample_bytree': 0.4783387354506317, 'subsample': 0.816448948728171, 'reg_alpha': 0.007916271442092818, 'reg_lambda': 0.0028036577609040045}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:38:15,792] Trial 30 finished with value: 109400.07692867496 and parameters: {'learning_rate': 0.03076498906811101, 'max_depth': 6, 'colsample_bytree': 0.40007709164692895, 'subsample': 0.7851459272073188, 'reg_alpha': 0.0005302687853071319, 'reg_lambda': 1.9751162506774606e-06}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:38:27,829] Trial 31 finished with value: 106373.36689228183 and parameters: {'learning_rate': 0.036481832136364065, 'max_depth': 4, 'colsample_bytree': 0.4739586166080843, 'subsample': 0.7672109679193803, 'reg_alpha': 0.0001315645326453667, 'reg_lambda': 2.290438401675073e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:38:41,168] Trial 32 finished with value: 110853.81047126887 and parameters: {'learning_rate': 0.020554772998353975, 'max_depth': 5, 'colsample_bytree': 0.844432254048047, 'subsample': 0.8308922265828151, 'reg_alpha': 2.4828995643840925, 'reg_lambda': 1.1939001288687391e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:38:51,641] Trial 33 finished with value: 115790.39822023241 and parameters: {'learning_rate': 0.0753073662602966, 'max_depth': 3, 'colsample_bytree': 0.5567261700429147, 'subsample': 0.7956238927509016, 'reg_alpha': 7.148801465291502, 'reg_lambda': 3.8966509063474914}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:39:06,510] Trial 34 finished with value: 108244.99382419494 and parameters: {'learning_rate': 0.01408649696620261, 'max_depth': 5, 'colsample_bytree': 0.43261782903561397, 'subsample': 0.8096746031382838, 'reg_alpha': 0.00136909249372436, 'reg_lambda': 5.577414192780039e-08}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:39:19,239] Trial 35 finished with value: 107569.96458119711 and parameters: {'learning_rate': 0.017878559668482657, 'max_depth': 4, 'colsample_bytree': 0.7558464337966848, 'subsample': 0.7744536044188933, 'reg_alpha': 0.009490427501513665, 'reg_lambda': 0.035932027659348734}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:39:35,756] Trial 36 finished with value: 108763.15611455932 and parameters: {'learning_rate': 0.02698702271878889, 'max_depth': 6, 'colsample_bytree': 0.6359838831061406, 'subsample': 0.603084763265129, 'reg_alpha': 2.5363794906135965e-05, 'reg_lambda': 5.152045973262267e-07}. Best is trial 6 with value: 105848.68656719364.\n",
      "[I 2026-01-01 20:39:47,933] Trial 37 finished with value: 105464.18417643025 and parameters: {'learning_rate': 0.04404571686281758, 'max_depth': 4, 'colsample_bytree': 0.783415780750639, 'subsample': 0.8585310841443085, 'reg_alpha': 1.5498269678902607e-06, 'reg_lambda': 8.706433948524363e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:40:02,342] Trial 38 finished with value: 107858.63815198113 and parameters: {'learning_rate': 0.042242968821851135, 'max_depth': 5, 'colsample_bytree': 0.8026839638067398, 'subsample': 0.8718090415945842, 'reg_alpha': 7.355732578098474e-07, 'reg_lambda': 2.3949347568174593e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:40:18,813] Trial 39 finished with value: 108764.6577891918 and parameters: {'learning_rate': 0.037744275337619836, 'max_depth': 6, 'colsample_bytree': 0.5480290073146324, 'subsample': 0.8549904437947136, 'reg_alpha': 6.1356423774285504e-06, 'reg_lambda': 0.0028166011098221086}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:40:28,962] Trial 40 finished with value: 109148.61794819025 and parameters: {'learning_rate': 0.04582381631947715, 'max_depth': 3, 'colsample_bytree': 0.4859843319181127, 'subsample': 0.8833164182474367, 'reg_alpha': 3.6357609643396544e-08, 'reg_lambda': 0.00015372722062548954}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:40:41,226] Trial 41 finished with value: 106056.4052191097 and parameters: {'learning_rate': 0.05433658522052486, 'max_depth': 4, 'colsample_bytree': 0.8617401023982117, 'subsample': 0.8343100252836033, 'reg_alpha': 0.1736138389113217, 'reg_lambda': 0.0007772658389780939}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:40:54,402] Trial 42 finished with value: 107612.97884549056 and parameters: {'learning_rate': 0.06405121761058107, 'max_depth': 4, 'colsample_bytree': 0.8660481418851617, 'subsample': 0.8345085601983502, 'reg_alpha': 8.445881020983635e-07, 'reg_lambda': 0.0009435367641132839}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:41:08,057] Trial 43 finished with value: 108728.63070967095 and parameters: {'learning_rate': 0.05407492070733571, 'max_depth': 4, 'colsample_bytree': 0.8233117596755394, 'subsample': 0.8488572850739777, 'reg_alpha': 2.4738723322551934e-06, 'reg_lambda': 0.01738743662608551}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:41:24,452] Trial 44 finished with value: 110368.04175122434 and parameters: {'learning_rate': 0.03199792859960998, 'max_depth': 5, 'colsample_bytree': 0.8838229927776784, 'subsample': 0.8279184271562927, 'reg_alpha': 0.18858092226817996, 'reg_lambda': 0.011676516824095455}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:41:34,756] Trial 45 finished with value: 112402.65448822817 and parameters: {'learning_rate': 0.023784995644968818, 'max_depth': 3, 'colsample_bytree': 0.8971380288472229, 'subsample': 0.8048102426395698, 'reg_alpha': 0.0014531790534965951, 'reg_lambda': 5.816619178979894e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:41:42,201] Trial 46 finished with value: 115507.86861508613 and parameters: {'learning_rate': 0.0051417283917633665, 'max_depth': 5, 'colsample_bytree': 0.8569870328757357, 'subsample': 0.8612672453258132, 'reg_alpha': 1.2426181290630499e-05, 'reg_lambda': 0.0007688648610120982}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:41:56,179] Trial 47 finished with value: 112328.92044349042 and parameters: {'learning_rate': 0.027865289946176486, 'max_depth': 7, 'colsample_bytree': 0.7550002625403066, 'subsample': 0.895461849829106, 'reg_alpha': 1.9853282902077916e-07, 'reg_lambda': 2.017942258358338e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:01,834] Trial 48 finished with value: 106697.22815518687 and parameters: {'learning_rate': 0.042006578873342235, 'max_depth': 4, 'colsample_bytree': 0.8189995891207407, 'subsample': 0.8415431042133915, 'reg_alpha': 5.311401094799494e-05, 'reg_lambda': 1.5595191343300477e-06}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:06,738] Trial 49 finished with value: 107369.69017371708 and parameters: {'learning_rate': 0.018968412560011604, 'max_depth': 4, 'colsample_bytree': 0.6743252033641374, 'subsample': 0.8828359963426589, 'reg_alpha': 0.0002010474673734662, 'reg_lambda': 8.460998185439369e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:13,299] Trial 50 finished with value: 108442.57997668628 and parameters: {'learning_rate': 0.051100509158937935, 'max_depth': 5, 'colsample_bytree': 0.4360636987592104, 'subsample': 0.7961593566234199, 'reg_alpha': 0.015050331035402244, 'reg_lambda': 0.000304835778775731}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:19,089] Trial 51 finished with value: 110019.79283747084 and parameters: {'learning_rate': 0.08739259335877622, 'max_depth': 4, 'colsample_bytree': 0.8442958198906989, 'subsample': 0.7207264200641892, 'reg_alpha': 0.25633799883743497, 'reg_lambda': 1.0698392501623398e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:24,603] Trial 52 finished with value: 106247.52744417162 and parameters: {'learning_rate': 0.06458173673011895, 'max_depth': 4, 'colsample_bytree': 0.7848440697682617, 'subsample': 0.6553121080794744, 'reg_alpha': 0.0031441083271977594, 'reg_lambda': 2.8870583884027532e-08}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:28,799] Trial 53 finished with value: 109190.96343562502 and parameters: {'learning_rate': 0.07328884907116318, 'max_depth': 3, 'colsample_bytree': 0.76195437208906, 'subsample': 0.7630629564339089, 'reg_alpha': 0.07848117185641479, 'reg_lambda': 5.512762002604209e-07}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:34,442] Trial 54 finished with value: 109668.2391943994 and parameters: {'learning_rate': 0.036037966413477875, 'max_depth': 4, 'colsample_bytree': 0.809126423684408, 'subsample': 0.811179269887964, 'reg_alpha': 1.5620003252457815, 'reg_lambda': 1.5360330648919679e-07}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:43,066] Trial 55 finished with value: 107612.03204103155 and parameters: {'learning_rate': 0.08886632986852408, 'max_depth': 5, 'colsample_bytree': 0.8416113802836468, 'subsample': 0.8246924670374569, 'reg_alpha': 0.17548445940573976, 'reg_lambda': 0.3724986574160438}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:42:55,421] Trial 56 finished with value: 105666.16593782515 and parameters: {'learning_rate': 0.06137856240426421, 'max_depth': 4, 'colsample_bytree': 0.4151739146313707, 'subsample': 0.7497916585277086, 'reg_alpha': 0.019151486783840464, 'reg_lambda': 3.254452714529973e-05}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:43:11,708] Trial 57 finished with value: 108613.25149354474 and parameters: {'learning_rate': 0.01595696203597933, 'max_depth': 5, 'colsample_bytree': 0.4180068820726157, 'subsample': 0.7464787688964212, 'reg_alpha': 0.0009750685391981535, 'reg_lambda': 3.6886380978773772e-06}. Best is trial 37 with value: 105464.18417643025.\n",
      "[I 2026-01-01 20:43:39,126] Trial 58 finished with value: 105127.56380702446 and parameters: {'learning_rate': 0.04167077820979329, 'max_depth': 4, 'colsample_bytree': 0.5081645215590987, 'subsample': 0.7044127278595476, 'reg_alpha': 0.0064105077359695875, 'reg_lambda': 2.8820295372118168e-05}. Best is trial 58 with value: 105127.56380702446.\n",
      "[I 2026-01-01 20:43:57,518] Trial 59 finished with value: 105444.07397288858 and parameters: {'learning_rate': 0.061229432940896555, 'max_depth': 4, 'colsample_bytree': 0.5064256956275804, 'subsample': 0.6902368530188193, 'reg_alpha': 0.01205405784727026, 'reg_lambda': 3.372405019376821e-05}. Best is trial 58 with value: 105127.56380702446.\n"
     ]
    }
   ],
   "source": [
    "# --- OPTIONAL: Clear CUDA Cache ---\n",
    "try:\n",
    "    import torch\n",
    "    def clear_gpu():\n",
    "        torch.cuda.empty_cache()\n",
    "except ImportError:\n",
    "    def clear_gpu():\n",
    "        pass\n",
    "\n",
    "def objective(trial):\n",
    "    # SAFETY: Aggressively clean memory\n",
    "    gc.collect()\n",
    "    clear_gpu()\n",
    "    \n",
    "    # --- STEP 1: LOG TRANSFORM TARGETS ---\n",
    "    # We use np.log1p (log(1+x)) to be safe against 0s, \n",
    "    # but np.log is fine if all y > 0.\n",
    "    y_train_log = np.log(y_train) \n",
    "    y_valid_log = np.log(y_valid)\n",
    "    \n",
    "    param = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7), \n",
    "        'max_bin': 128, \n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),  \n",
    "        'device': 'cuda', \n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = xgb.XGBRegressor(**param)\n",
    "        \n",
    "        # --- STEP 2: TRAIN ON LOG TARGETS ---\n",
    "        model.fit(\n",
    "            X_train_final, y_train_log,\n",
    "            # Validation set must also be log-transformed for internal metrics to make sense\n",
    "            eval_set=[(X_valid_final, y_valid_log)], \n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # --- STEP 3: PREDICT (Returns Log Values) ---\n",
    "        preds_log = model.predict(X_valid_final)\n",
    "        \n",
    "        # --- STEP 4: INVERSE TRANSFORM (Exp) ---\n",
    "        # Convert predictions back to original currency/scale\n",
    "        preds_original = np.exp(preds_log)\n",
    "        \n",
    "        # --- STEP 5: CALCULATE RMSE ON ORIGINAL SCALE ---\n",
    "        # We compare 'preds_original' vs 'y_valid' (the raw data)\n",
    "        rmse = np.sqrt(mean_squared_error(y_valid, preds_original))\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        clear_gpu()\n",
    "        \n",
    "        return rmse\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with error: {e}\")\n",
    "        return float('inf') \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c4cb4",
   "metadata": {},
   "source": [
    "Trial 15 finished with value: 112267.19670500373 and parameters: {'learning_rate': 0.0486074461648618, 'max_depth': 4, 'colsample_bytree': 0.6148933024076417, 'subsample': 0.7056132339243473, 'reg_alpha': 0.00032494489089217554, 'reg_lambda': 0.0013207991543530502}. Best is trial 15 with value: 112267.19670500373.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4039a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.51004\n",
      "[100]\tvalidation_0-rmse:0.18793\n",
      "[200]\tvalidation_0-rmse:0.17174\n",
      "[300]\tvalidation_0-rmse:0.16783\n",
      "[400]\tvalidation_0-rmse:0.16560\n",
      "[500]\tvalidation_0-rmse:0.16417\n",
      "[600]\tvalidation_0-rmse:0.16310\n",
      "[700]\tvalidation_0-rmse:0.16256\n",
      "[800]\tvalidation_0-rmse:0.16200\n",
      "[900]\tvalidation_0-rmse:0.16165\n",
      "[1000]\tvalidation_0-rmse:0.16127\n",
      "[1100]\tvalidation_0-rmse:0.16106\n",
      "[1200]\tvalidation_0-rmse:0.16077\n",
      "[1300]\tvalidation_0-rmse:0.16084\n",
      "[1400]\tvalidation_0-rmse:0.16082\n",
      "[1500]\tvalidation_0-rmse:0.16085\n",
      "[1600]\tvalidation_0-rmse:0.16087\n",
      "[1700]\tvalidation_0-rmse:0.16094\n",
      "[1800]\tvalidation_0-rmse:0.16077\n",
      "[1900]\tvalidation_0-rmse:0.16085\n",
      "[2000]\tvalidation_0-rmse:0.16097\n",
      "[2100]\tvalidation_0-rmse:0.16096\n",
      "[2200]\tvalidation_0-rmse:0.16100\n",
      "[2300]\tvalidation_0-rmse:0.16097\n",
      "[2400]\tvalidation_0-rmse:0.16095\n",
      "[2500]\tvalidation_0-rmse:0.16111\n",
      "[2600]\tvalidation_0-rmse:0.16112\n",
      "[2700]\tvalidation_0-rmse:0.16120\n",
      "[2800]\tvalidation_0-rmse:0.16114\n",
      "[2900]\tvalidation_0-rmse:0.16120\n",
      "[3000]\tvalidation_0-rmse:0.16127\n",
      "[3100]\tvalidation_0-rmse:0.16127\n",
      "[3200]\tvalidation_0-rmse:0.16133\n",
      "[3300]\tvalidation_0-rmse:0.16146\n",
      "[3400]\tvalidation_0-rmse:0.16146\n",
      "[3500]\tvalidation_0-rmse:0.16149\n",
      "[3600]\tvalidation_0-rmse:0.16154\n",
      "[3700]\tvalidation_0-rmse:0.16156\n",
      "[3800]\tvalidation_0-rmse:0.16161\n",
      "[3900]\tvalidation_0-rmse:0.16165\n",
      "[4000]\tvalidation_0-rmse:0.16176\n",
      "[4100]\tvalidation_0-rmse:0.16180\n",
      "[4200]\tvalidation_0-rmse:0.16180\n",
      "[4300]\tvalidation_0-rmse:0.16183\n",
      "[4400]\tvalidation_0-rmse:0.16181\n",
      "[4500]\tvalidation_0-rmse:0.16188\n",
      "[4600]\tvalidation_0-rmse:0.16185\n",
      "[4700]\tvalidation_0-rmse:0.16186\n",
      "[4800]\tvalidation_0-rmse:0.16183\n",
      "[4900]\tvalidation_0-rmse:0.16185\n",
      "[4999]\tvalidation_0-rmse:0.16194\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "# Add the fixed parameters back (since Optuna only holds the variable ones)\n",
    "best_params['n_estimators'] = 5000\n",
    "best_params['device'] = 'cuda'\n",
    "best_params['n_jobs'] = -1\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "final_model = xgb.XGBRegressor(**best_params)\n",
    "final_model.fit(\n",
    "    X_train_final, y_train_log, \n",
    "    eval_set=[(X_valid_final, y_valid_log)], \n",
    "    verbose=100\n",
    ")\n",
    "preds_log = final_model.predict(X_valid_final)\n",
    "prd = np.exp(preds_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168af7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "RMSE:  107008.3203125\n",
      "R2 score:  0.9055907130241394\n"
     ]
    }
   ],
   "source": [
    "xgb_rmse = root_mean_squared_error(y_valid, prd)\n",
    "xgb_r2 = r2_score(y_true=y_valid, y_pred=prd)\n",
    "print(\"XGBoost\")\n",
    "print(\"RMSE: \", xgb_rmse)\n",
    "print(\"R2 score: \", xgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5044a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "RMSE:  108720.703125\n",
      "R2 score:  0.9025450348854065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "xgb_rmse = root_mean_squared_error(y_valid, prd)\n",
    "xgb_r2 = r2_score(y_true=y_valid, y_pred=prd)\n",
    "print(\"XGBoost\")\n",
    "print(\"RMSE: \", xgb_rmse)\n",
    "print(\"R2 score: \", xgb_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96148824",
   "metadata": {},
   "source": [
    "Lets try ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b7bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 419791536128.0000 - mae: 536569.3750 - val_loss: 411199700992.0000 - val_mae: 538634.6875 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 417533591552.0000 - mae: 535234.6250 - val_loss: 410343473152.0000 - val_mae: 538639.0625 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 410690519040.0000 - mae: 531011.3125 - val_loss: 403659128832.0000 - val_mae: 534504.6875 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 397624442880.0000 - mae: 522674.9375 - val_loss: 381140893696.0000 - val_mae: 520352.2500 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 377707003904.0000 - mae: 509510.2500 - val_loss: 346579927040.0000 - val_mae: 495542.0000 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 350872502272.0000 - mae: 490994.5625 - val_loss: 353280425984.0000 - val_mae: 504842.7500 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 319175393280.0000 - mae: 467389.8750 - val_loss: 293084495872.0000 - val_mae: 456773.1250 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 283493597184.0000 - mae: 438976.5312 - val_loss: 305702830080.0000 - val_mae: 475969.2188 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 246135095296.0000 - mae: 405594.7500 - val_loss: 194080587776.0000 - val_mae: 363850.7812 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 207579152384.0000 - mae: 369040.0938 - val_loss: 261822382080.0000 - val_mae: 448590.5938 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 172975046656.0000 - mae: 329710.1875 - val_loss: 202758848512.0000 - val_mae: 388305.1250 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 141246922752.0000 - mae: 291588.0938 - val_loss: 106374184960.0000 - val_mae: 265949.0625 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 114486501376.0000 - mae: 253824.7656 - val_loss: 88441028608.0000 - val_mae: 229416.9688 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 94500233216.0000 - mae: 220637.3906 - val_loss: 90214162432.0000 - val_mae: 244671.5469 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 77072457728.0000 - mae: 190249.2344 - val_loss: 73706545152.0000 - val_mae: 205456.9844 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 66018836480.0000 - mae: 169912.9062 - val_loss: 65895636992.0000 - val_mae: 198672.2500 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 58777591808.0000 - mae: 154852.2656 - val_loss: 54758895616.0000 - val_mae: 166496.2188 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 54172454912.0000 - mae: 147344.7656 - val_loss: 48323506176.0000 - val_mae: 159807.1562 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 52439236608.0000 - mae: 144731.4531 - val_loss: 43484360704.0000 - val_mae: 144473.5938 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 50929541120.0000 - mae: 143276.5781 - val_loss: 43790987264.0000 - val_mae: 144195.9375 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 49203699712.0000 - mae: 140602.0469 - val_loss: 41411870720.0000 - val_mae: 131375.4844 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 49744728064.0000 - mae: 139969.0781 - val_loss: 39494561792.0000 - val_mae: 132241.7812 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 49950760960.0000 - mae: 141081.7188 - val_loss: 41987461120.0000 - val_mae: 139175.3750 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 48941101056.0000 - mae: 140548.3594 - val_loss: 44963446784.0000 - val_mae: 134938.8438 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 48604991488.0000 - mae: 140934.3594 - val_loss: 35124736000.0000 - val_mae: 120281.4531 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 49930981376.0000 - mae: 140531.8906 - val_loss: 37205733376.0000 - val_mae: 120233.4531 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 48319873024.0000 - mae: 140318.0312 - val_loss: 37966458880.0000 - val_mae: 126672.2188 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 47964155904.0000 - mae: 138095.2500 - val_loss: 40175747072.0000 - val_mae: 127987.2969 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 47486124032.0000 - mae: 138884.5938 - val_loss: 39038226432.0000 - val_mae: 133569.9688 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 49000841216.0000 - mae: 139250.7812 - val_loss: 40822665216.0000 - val_mae: 130278.7344 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 44995854336.0000 - mae: 135632.7188 - val_loss: 37121404928.0000 - val_mae: 123035.6016 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 47845490688.0000 - mae: 136876.4844 - val_loss: 34453155840.0000 - val_mae: 119387.2422 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 47930462208.0000 - mae: 138192.1250 - val_loss: 37528444928.0000 - val_mae: 122289.2500 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 48670879744.0000 - mae: 139372.7344 - val_loss: 40509931520.0000 - val_mae: 126095.6484 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 47006461952.0000 - mae: 138363.2969 - val_loss: 36081045504.0000 - val_mae: 118629.2188 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 45919297536.0000 - mae: 136589.7344 - val_loss: 40208363520.0000 - val_mae: 128311.7578 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 46423425024.0000 - mae: 136423.0469 - val_loss: 39263440896.0000 - val_mae: 130682.4688 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 47209742336.0000 - mae: 137619.0312 - val_loss: 34962518016.0000 - val_mae: 119227.7188 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 45630480384.0000 - mae: 135195.3438 - val_loss: 35267538944.0000 - val_mae: 119766.6953 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 47178682368.0000 - mae: 135155.6094 - val_loss: 36326019072.0000 - val_mae: 120589.9375 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 46070681600.0000 - mae: 137420.9531 - val_loss: 37084319744.0000 - val_mae: 119878.1953 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 47021834240.0000 - mae: 136764.2344 - val_loss: 36172300288.0000 - val_mae: 119696.6094 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 45735669760.0000 - mae: 135866.7656 - val_loss: 35477950464.0000 - val_mae: 118879.5547 - learning_rate: 1.2500e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 45404495872.0000 - mae: 135975.3906 - val_loss: 35376590848.0000 - val_mae: 119240.5391 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 45919571968.0000 - mae: 136110.0781 - val_loss: 34897788928.0000 - val_mae: 119081.8516 - learning_rate: 1.2500e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 46051209216.0000 - mae: 136723.6875 - val_loss: 34864435200.0000 - val_mae: 118998.6719 - learning_rate: 1.2500e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 47936073728.0000 - mae: 138448.6406 - val_loss: 36533334016.0000 - val_mae: 119705.3359 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 44740812800.0000 - mae: 135516.8906 - val_loss: 35211579392.0000 - val_mae: 119080.7734 - learning_rate: 6.2500e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 45616955392.0000 - mae: 135919.8750 - val_loss: 35506155520.0000 - val_mae: 119171.5625 - learning_rate: 6.2500e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 47487442944.0000 - mae: 137191.5156 - val_loss: 35212267520.0000 - val_mae: 119564.5078 - learning_rate: 6.2500e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 45994283008.0000 - mae: 136096.7500 - val_loss: 35044327424.0000 - val_mae: 119027.6406 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 46293938176.0000 - mae: 136669.7500 - val_loss: 35968688128.0000 - val_mae: 119144.3281 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "input_tab = layers.Input(shape=(22,), name='Tabular_Input')\n",
    "input_emb = layers.Input(shape=(100,), name='Embedding_Input')\n",
    "\n",
    "x_tab = layers.Dense(32, kernel_regularizer=regularizers.l2(0.001))(input_tab)\n",
    "x_tab = layers.BatchNormalization()(x_tab)\n",
    "x_tab = layers.Activation('relu')(x_tab)\n",
    "x_tab = layers.Dropout(0.2)(x_tab) \n",
    "\n",
    "x_emb = layers.Dense(64, kernel_regularizer=regularizers.l2(0.001))(input_emb)\n",
    "x_emb = layers.BatchNormalization()(x_emb)\n",
    "x_emb = layers.Activation('relu')(x_emb)\n",
    "x_emb = layers.Dropout(0.4)(x_emb)\n",
    "\n",
    "concat = layers.Concatenate()([x_tab, x_emb])\n",
    "\n",
    "x = layers.Dense(64, kernel_regularizer=regularizers.l2(0.001))(concat)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(32, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "output = layers.Dense(1, name='output')(x)\n",
    "\n",
    "model = models.Model(inputs=[input_tab, input_emb], outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001) \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "my_callbacks = [\n",
    "    callbacks.EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss'),\n",
    "    callbacks.ReduceLROnPlateau(factor=0.5, patience=5, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [X_tabular, X_embed_pca], y_train,\n",
    "    validation_data=([X_valid_tabular, X_valid_embed_pca], y_valid),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758ee63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m406/406\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "------------------------------\n",
      "ANN PERFORMANCE REPORT\n",
      "------------------------------\n",
      "Train RMSE : 160,921\n",
      "Valid RMSE : 185,616\n",
      "------------------------------\n",
      "Train R   : 0.8037\n",
      "Valid R   : 0.7159\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict([X_tabular, X_embed_pca]).flatten()\n",
    "y_pred_valid = model.predict([X_valid_tabular, X_valid_embed_pca]).flatten()\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "valid_r2 = r2_score(y_valid, y_pred_valid)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"ANN PERFORMANCE REPORT\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Train RMSE : {train_rmse:,.0f}\")\n",
    "print(f\"Valid RMSE : {valid_rmse:,.0f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Train R   : {train_r2:.4f}\")\n",
    "print(f\"Valid R   : {valid_r2:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ff185",
   "metadata": {},
   "source": [
    "Lets try with full embeddings now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41c6d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 419799105536.0000 - mae: 536602.3750 - val_loss: 413168631808.0000 - val_mae: 540466.8750 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 418310848512.0000 - mae: 535853.7500 - val_loss: 411260354560.0000 - val_mae: 539407.1250 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 413518200832.0000 - mae: 533283.1250 - val_loss: 408183570432.0000 - val_mae: 538097.5000 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 403593592832.0000 - mae: 527560.4375 - val_loss: 402980044800.0000 - val_mae: 535557.4375 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 387250814976.0000 - mae: 517904.3750 - val_loss: 398591426560.0000 - val_mae: 533585.3125 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 363931074560.0000 - mae: 503116.4688 - val_loss: 398448787456.0000 - val_mae: 531657.0000 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 334779449344.0000 - mae: 483563.7188 - val_loss: 357592236032.0000 - val_mae: 507963.5000 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 300201934848.0000 - mae: 458891.7812 - val_loss: 344810553344.0000 - val_mae: 504235.0000 - learning_rate: 0.0010\n",
      "Epoch 9/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 262475120640.0000 - mae: 429546.9688 - val_loss: 308561870848.0000 - val_mae: 481705.5000 - learning_rate: 0.0010\n",
      "Epoch 10/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 223413895168.0000 - mae: 395648.9375 - val_loss: 284849340416.0000 - val_mae: 456376.9375 - learning_rate: 0.0010\n",
      "Epoch 11/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 184257150976.0000 - mae: 358478.1875 - val_loss: 177183932416.0000 - val_mae: 368919.0938 - learning_rate: 0.0010\n",
      "Epoch 12/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 148144095232.0000 - mae: 319437.6250 - val_loss: 222669340672.0000 - val_mae: 390453.8125 - learning_rate: 0.0010\n",
      "Epoch 13/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 117780676608.0000 - mae: 278778.7188 - val_loss: 157240393728.0000 - val_mae: 343865.4375 - learning_rate: 0.0010\n",
      "Epoch 14/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 92097241088.0000 - mae: 240364.2344 - val_loss: 118715580416.0000 - val_mae: 271446.6250 - learning_rate: 0.0010\n",
      "Epoch 15/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 71621337088.0000 - mae: 206469.5625 - val_loss: 118636412928.0000 - val_mae: 276992.2500 - learning_rate: 0.0010\n",
      "Epoch 16/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 56708427776.0000 - mae: 176960.6562 - val_loss: 106363428864.0000 - val_mae: 244005.3906 - learning_rate: 0.0010\n",
      "Epoch 17/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 45962051584.0000 - mae: 155408.4531 - val_loss: 95124258816.0000 - val_mae: 215673.6875 - learning_rate: 0.0010\n",
      "Epoch 18/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 40263241728.0000 - mae: 138845.8125 - val_loss: 59782242304.0000 - val_mae: 181518.9688 - learning_rate: 0.0010\n",
      "Epoch 19/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 32328128512.0000 - mae: 127007.9062 - val_loss: 76195913728.0000 - val_mae: 197016.2969 - learning_rate: 0.0010\n",
      "Epoch 20/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 33237499904.0000 - mae: 122470.4141 - val_loss: 39027548160.0000 - val_mae: 132719.4219 - learning_rate: 0.0010\n",
      "Epoch 21/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 30403686400.0000 - mae: 116612.6953 - val_loss: 32620957696.0000 - val_mae: 115409.8125 - learning_rate: 0.0010\n",
      "Epoch 22/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 28521775104.0000 - mae: 113112.7031 - val_loss: 63036280832.0000 - val_mae: 170164.6719 - learning_rate: 0.0010\n",
      "Epoch 23/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 26847315968.0000 - mae: 111606.7422 - val_loss: 35433324544.0000 - val_mae: 125679.2266 - learning_rate: 0.0010\n",
      "Epoch 24/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 26510301184.0000 - mae: 110239.0312 - val_loss: 46789382144.0000 - val_mae: 137926.0938 - learning_rate: 0.0010\n",
      "Epoch 25/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 27913222144.0000 - mae: 111505.9375 - val_loss: 33144705024.0000 - val_mae: 118241.4297 - learning_rate: 0.0010\n",
      "Epoch 26/150\n",
      "\u001b[1m202/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 23821381824.6337 - mae: 107065.7052\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 25812127744.0000 - mae: 107611.8828 - val_loss: 34698276864.0000 - val_mae: 130380.7656 - learning_rate: 0.0010\n",
      "Epoch 27/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 23371857920.0000 - mae: 103813.4062 - val_loss: 31317776384.0000 - val_mae: 111722.5234 - learning_rate: 2.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22779412480.0000 - mae: 102785.8594 - val_loss: 32897677312.0000 - val_mae: 114787.9375 - learning_rate: 2.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 23327131648.0000 - mae: 103519.0469 - val_loss: 30594789376.0000 - val_mae: 110613.0703 - learning_rate: 2.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 23059331072.0000 - mae: 103518.2109 - val_loss: 29764683776.0000 - val_mae: 108924.5078 - learning_rate: 2.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 22893039616.0000 - mae: 102788.9375 - val_loss: 30156189696.0000 - val_mae: 110648.8438 - learning_rate: 2.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21502078976.0000 - mae: 99829.2031 - val_loss: 30494593024.0000 - val_mae: 110198.9062 - learning_rate: 2.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22559086592.0000 - mae: 101972.3125 - val_loss: 30952763392.0000 - val_mae: 110478.7188 - learning_rate: 2.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22404696064.0000 - mae: 101848.3203 - val_loss: 30651807744.0000 - val_mae: 110385.7109 - learning_rate: 2.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 21059198184.0394 - mae: 98414.6909\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21901383680.0000 - mae: 100854.8906 - val_loss: 31200643072.0000 - val_mae: 110653.0703 - learning_rate: 2.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 20904914944.0000 - mae: 99097.8594 - val_loss: 30546477056.0000 - val_mae: 109675.2812 - learning_rate: 4.0000e-05\n",
      "Epoch 37/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22747820032.0000 - mae: 102432.3516 - val_loss: 30164058112.0000 - val_mae: 109573.8359 - learning_rate: 4.0000e-05\n",
      "Epoch 38/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 21663313920.0000 - mae: 100378.7969 - val_loss: 30096236544.0000 - val_mae: 109785.2500 - learning_rate: 4.0000e-05\n",
      "Epoch 39/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21664995328.0000 - mae: 99778.2656 - val_loss: 29866584064.0000 - val_mae: 109429.8438 - learning_rate: 4.0000e-05\n",
      "Epoch 40/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 27985081016.1182 - mae: 103906.6481\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22596423680.0000 - mae: 100696.2422 - val_loss: 30116454400.0000 - val_mae: 109622.5938 - learning_rate: 4.0000e-05\n",
      "Epoch 41/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 20863660032.0000 - mae: 99407.8672 - val_loss: 30133395456.0000 - val_mae: 109567.2734 - learning_rate: 8.0000e-06\n",
      "Epoch 42/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 20625981440.0000 - mae: 98114.4844 - val_loss: 30172960768.0000 - val_mae: 109232.4375 - learning_rate: 8.0000e-06\n",
      "Epoch 43/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 22477125632.0000 - mae: 100706.5312 - val_loss: 30027208704.0000 - val_mae: 109583.2188 - learning_rate: 8.0000e-06\n",
      "Epoch 44/150\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21790359552.0000 - mae: 99764.9922 - val_loss: 30245527552.0000 - val_mae: 109761.5312 - learning_rate: 8.0000e-06\n",
      "Epoch 45/150\n",
      "\u001b[1m200/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 27981579939.8400 - mae: 101319.8572\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 21507663872.0000 - mae: 99596.4453 - val_loss: 30072080384.0000 - val_mae: 109399.6719 - learning_rate: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "input_dim_emb = X_embed.shape[1]\n",
    "input_dim_tab = X_tabular.shape[1] \n",
    "\n",
    "# --- INPUTS ---\n",
    "input_emb = layers.Input(shape=(input_dim_emb,), name='Full_Embeddings')\n",
    "input_tab = layers.Input(shape=(input_dim_tab,), name='Tabular_Data')\n",
    "\n",
    "x_emb = layers.Dense(512)(input_emb)\n",
    "x_emb = layers.BatchNormalization()(x_emb) \n",
    "x_emb = layers.Activation('swish')(x_emb)  # 'swish' often beats 'relu' for deep nets\n",
    "x_emb = layers.Dropout(0.3)(x_emb) \n",
    "\n",
    "x_emb = layers.Dense(256)(x_emb)\n",
    "x_emb = layers.BatchNormalization()(x_emb)\n",
    "x_emb = layers.Activation('swish')(x_emb)\n",
    "x_emb = layers.Dropout(0.3)(x_emb)\n",
    "\n",
    "x_emb = layers.Dense(128)(x_emb)\n",
    "x_emb = layers.BatchNormalization()(x_emb)\n",
    "x_emb = layers.Activation('swish')(x_emb)\n",
    "\n",
    "x_tab = layers.Dense(64)(input_tab)\n",
    "x_tab = layers.BatchNormalization()(x_tab)\n",
    "x_tab = layers.Activation('swish')(x_tab)\n",
    "x_tab = layers.Dropout(0.1)(x_tab) # Low dropout, these features are precious\n",
    "\n",
    "concat = layers.Concatenate()([x_tab, x_emb])\n",
    "\n",
    "x = layers.Dense(256)(concat)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('swish')(x)\n",
    "x = layers.Dropout(0.4)(x) # Higher dropout here to prevent memorization\n",
    "\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('swish')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Activation('swish')(x)\n",
    "\n",
    "output = layers.Dense(1, name='output')(x)\n",
    "\n",
    "model = models.Model(inputs=[input_emb, input_tab], outputs=output)\n",
    "\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=0.004)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "my_callbacks = [\n",
    "    callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),\n",
    "    callbacks.ReduceLROnPlateau(factor=0.2, patience=5, monitor='val_loss', verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [X_embed, X_tabular], y_train,\n",
    "    validation_data=([X_valid_embed, X_valid_tabular], y_valid),\n",
    "    epochs=150, \n",
    "    batch_size=64,\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192a3c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "========================================\n",
      "   COMPLEX ANN EVALUATION REPORT\n",
      "========================================\n",
      "Metric     | Train        | Validation  \n",
      "----------------------------------------\n",
      "RMSE       | 96,440.93    | 172,524.42\n",
      "MAE        | 70,165.66    | 108,924.51\n",
      "R         | 0.9295      | 0.7546\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Generate Predictions\n",
    "# Pass both inputs: [Full Embeddings, Tabular Data]\n",
    "print(\"Generating predictions...\")\n",
    "y_pred_train = model.predict([X_embed, X_tabular], verbose=0).flatten()\n",
    "y_pred_valid = model.predict([X_valid_embed, X_valid_tabular], verbose=0).flatten()\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "valid_r2 = r2_score(y_valid, y_pred_valid)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "valid_mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"   COMPLEX ANN EVALUATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"{'Metric':<10} | {'Train':<12} | {'Validation':<12}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'RMSE':<10} | {train_rmse:,.2f}    | {valid_rmse:,.2f}\")\n",
    "print(f\"{'MAE':<10} | {train_mae:,.2f}    | {valid_mae:,.2f}\")\n",
    "print(f\"{'R':<10} | {train_r2:.4f}      | {valid_r2:.4f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b2054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
